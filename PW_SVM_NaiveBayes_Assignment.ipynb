{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "05b3efc9",
      "metadata": {
        "id": "05b3efc9"
      },
      "source": [
        "# SVM & Naive Bayes – Complete Theoretical & Practical Q&A"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927b287d",
      "metadata": {
        "id": "927b287d"
      },
      "source": [
        "## Part 1 – Theoretical Answers (Q1–Q20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcf864da",
      "metadata": {
        "id": "bcf864da"
      },
      "source": [
        "**1. What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "SVM is a powerful supervised learning algorithm that finds the **maximum-margin hyperplane** separating classes. It can work in high dimensions and uses kernels for non‑linear boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff53dfd",
      "metadata": {
        "id": "8ff53dfd"
      },
      "source": [
        "**2. What is the difference between Hard Margin and Soft Margin SVM?**\n",
        "\n",
        "**Hard Margin** assumes perfectly separable data and maximizes margin with zero misclassification (not robust to noise). **Soft Margin** allows some violations controlled by parameter **C**, balancing margin width and classification errors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c818e58d",
      "metadata": {
        "id": "c818e58d"
      },
      "source": [
        "**3. What is the mathematical intuition behind SVM?**\n",
        "\n",
        "SVM maximizes the geometric margin between classes. The decision function is based on a subset of training points (support vectors). The optimization is a convex quadratic problem with a unique global optimum."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c7381ef",
      "metadata": {
        "id": "2c7381ef"
      },
      "source": [
        "**4. What is the role of Lagrange Multipliers in SVM?**\n",
        "\n",
        "Lagrange multipliers transform the constrained primal optimization into the **dual problem**, enabling kernels and yielding sparse solutions where only support vectors have non‑zero multipliers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cfc0dd",
      "metadata": {
        "id": "96cfc0dd"
      },
      "source": [
        "**5. What are Support Vectors in SVM?**\n",
        "\n",
        "Training samples that lie on or inside the margin and determine the decision boundary. Removing non‑support vectors does not change the boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3adeef1c",
      "metadata": {
        "id": "3adeef1c"
      },
      "source": [
        "**6. What is a Support Vector Classifier (SVC)?**\n",
        "\n",
        "An SVM model used for **classification**. In scikit‑learn, `SVC` implements kernelized SVM for binary/multiclass tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e4bb366",
      "metadata": {
        "id": "4e4bb366"
      },
      "source": [
        "**7. What is a Support Vector Regressor (SVR)?**\n",
        "\n",
        "SVR applies SVM ideas to **regression**, using an ε‑insensitive loss to fit a function within a tube around data while maximizing flatness."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba20a235",
      "metadata": {
        "id": "ba20a235"
      },
      "source": [
        "**8. What is the Kernel Trick in SVM?**\n",
        "\n",
        "The kernel trick computes inner products in a high‑dimensional feature space **without explicit mapping**, enabling non‑linear decision boundaries (e.g., RBF, polynomial)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51cd79d3",
      "metadata": {
        "id": "51cd79d3"
      },
      "source": [
        "**9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.**\n",
        "\n",
        "- **Linear:** fast, works when data is roughly linearly separable.  \n",
        "- **Polynomial:** models interactions up to degree d; can overfit.  \n",
        "- **RBF:** popular default; creates flexible non‑linear boundaries controlled by γ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5310ba5",
      "metadata": {
        "id": "a5310ba5"
      },
      "source": [
        "**10. What is the effect of the C parameter in SVM?**\n",
        "\n",
        "**C** controls the trade‑off between maximizing margin and classification error. Small C → wider margin, more tolerance to misclassification; Large C → narrow margin, fit training data more strictly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf1a8025",
      "metadata": {
        "id": "cf1a8025"
      },
      "source": [
        "**11. What is the role of the Gamma parameter in RBF Kernel SVM?**\n",
        "\n",
        "**Gamma (γ)** controls the influence of a single training example in RBF. Low γ → smoother, broader influence; High γ → tighter, can overfit."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad6dabb4",
      "metadata": {
        "id": "ad6dabb4"
      },
      "source": [
        "**12. What is the Naive Bayes classifier, and why is it called 'Naive'?**\n",
        "\n",
        "Naive Bayes applies Bayes’ theorem with the **naive independence** assumption between features given the class; the assumption makes computation simple and scalable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d913673",
      "metadata": {
        "id": "3d913673"
      },
      "source": [
        "**13. What is Bayes’ Theorem?**\n",
        "\n",
        "$$ P(y|x)=\\frac{P(x|y)P(y)}{P(x)} $$ where \\(P(y|x)\\) is the posterior, \\(P(x|y)\\) likelihood, \\(P(y)\\) prior."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19dd0235",
      "metadata": {
        "id": "19dd0235"
      },
      "source": [
        "**14. Explain the differences between Gaussian NB, Multinomial NB, and Bernoulli NB.**\n",
        "\n",
        "**Gaussian NB:** continuous features assumed normal. **Multinomial NB:** counts/frequencies (e.g., bag‑of‑words). **Bernoulli NB:** binary features (0/1)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f30754ef",
      "metadata": {
        "id": "f30754ef"
      },
      "source": [
        "**15. When should you use Gaussian Naive Bayes over other variants?**\n",
        "\n",
        "When features are continuous and approximately Gaussian (e.g., medical measurements, sensor data)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5941a3fe",
      "metadata": {
        "id": "5941a3fe"
      },
      "source": [
        "**16. What are the key assumptions made by Naive Bayes?**\n",
        "\n",
        "Features are conditionally independent given the class; distributions match the chosen variant (Gaussian/Multinomial/Bernoulli)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be05e942",
      "metadata": {
        "id": "be05e942"
      },
      "source": [
        "**17. What are the advantages and disadvantages of Naive Bayes?**\n",
        "\n",
        "**Pros:** fast, works with small data, robust to irrelevant features, probabilistic outputs. **Cons:** independence assumption often violated; can be outperformed by more flexible models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd6674c6",
      "metadata": {
        "id": "fd6674c6"
      },
      "source": [
        "**18. Why is Naive Bayes a good choice for text classification?**\n",
        "\n",
        "Text features (word counts) are high‑dimensional and sparse; the independence assumption works surprisingly well; Multinomial NB is very efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d6e342",
      "metadata": {
        "id": "59d6e342"
      },
      "source": [
        "**19. Compare SVM and Naive Bayes for classification tasks.**\n",
        "\n",
        "SVM usually yields higher accuracy on complex boundaries but is slower on very large datasets; Naive Bayes is faster and good as a strong baseline, especially for text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb56e569",
      "metadata": {
        "id": "eb56e569"
      },
      "source": [
        "**20. How does Laplace Smoothing help in Naive Bayes?**\n",
        "\n",
        "Laplace (add‑one) smoothing avoids zero probabilities for unseen feature events by adding a small constant to counts before normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf01070",
      "metadata": {
        "id": "6bf01070"
      },
      "source": [
        "## Part 2 – Practical (Q21–Q46)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e49f546b",
      "metadata": {
        "id": "e49f546b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support, roc_auc_score, precision_recall_curve, mean_squared_error, mean_absolute_error\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Helper: simple confusion matrix plot (matplotlib only)\n",
        "def plot_confusion_matrix(cm, classes):\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest')\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           ylabel='True label', xlabel='Predicted label')\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], 'd'), ha='center', va='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Load standard datasets\n",
        "iris = datasets.load_iris()\n",
        "wine = datasets.load_wine()\n",
        "breast = datasets.load_breast_cancer()\n",
        "\n",
        "X_iris, y_iris = iris.data, iris.target\n",
        "X_wine, y_wine = wine.data, wine.target\n",
        "X_bc, y_bc = breast.data, breast.target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb4df29",
      "metadata": {
        "id": "7bb4df29"
      },
      "source": [
        "### 21. Train SVM on Iris & evaluate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb79c5e0",
      "metadata": {
        "id": "eb79c5e0"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
        "clf = SVC(kernel='rbf', gamma='scale')\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_test)\n",
        "print('Accuracy:', accuracy_score(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "081a22b2",
      "metadata": {
        "id": "081a22b2"
      },
      "source": [
        "### 22. Train SVM with Linear vs RBF on Wine and compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b23df4bf",
      "metadata": {
        "id": "b23df4bf"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, test_size=0.2, random_state=42)\n",
        "lin = SVC(kernel='linear').fit(X_train, y_train)\n",
        "rbf = SVC(kernel='rbf').fit(X_train, y_train)\n",
        "print('Linear:', lin.score(X_test, y_test))\n",
        "print('RBF:', rbf.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ea6e987",
      "metadata": {
        "id": "9ea6e987"
      },
      "source": [
        "### 23. Train SVR on synthetic 'housing' data & MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "381ab775",
      "metadata": {
        "id": "381ab775"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=1200, n_features=8, noise=15, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "reg = SVR(kernel='rbf')\n",
        "reg.fit(X_train, y_train)\n",
        "pred = reg.predict(X_test)\n",
        "print('MSE:', mean_squared_error(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9beb3997",
      "metadata": {
        "id": "9beb3997"
      },
      "source": [
        "### 24. SVM with Polynomial Kernel & visualize 2D decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d27fa76",
      "metadata": {
        "id": "3d27fa76"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=400, n_features=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "clf = SVC(kernel='poly', degree=3).fit(X, y)\n",
        "# Plot\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(X[:,0], X[:,1], c=y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d09407",
      "metadata": {
        "id": "34d09407"
      },
      "source": [
        "### 25. Gaussian NB on Breast Cancer – accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e704b6e",
      "metadata": {
        "id": "2e704b6e"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)\n",
        "gnb = GaussianNB().fit(X_train, y_train)\n",
        "print('Accuracy:', gnb.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43dd0d7c",
      "metadata": {
        "id": "43dd0d7c"
      },
      "source": [
        "### 26. Multinomial NB for text (20 Newsgroups fallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8e5aa8",
      "metadata": {
        "id": "4c8e5aa8"
      },
      "outputs": [],
      "source": [
        "# Try to fetch 20 Newsgroups; if unavailable (offline), use a small synthetic corpus\n",
        "try:\n",
        "    from sklearn.datasets import fetch_20newsgroups\n",
        "    train = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\n",
        "    test = fetch_20newsgroups(subset='test', remove=('headers','footers','quotes'))\n",
        "    X_train, X_test, y_train, y_test = train.data, test.data, train.target, test.target\n",
        "except Exception as e:\n",
        "    X_train = ['sports match win', 'election debate', 'new graphics card', 'religion philosophy', 'hockey team scores', 'government policy']\n",
        "    y_train = [0,1,2,3,0,1]\n",
        "    X_test  = ['team won match', 'policy debate', 'gpu benchmark']\n",
        "    y_test  = [0,1,2]\n",
        "vectorizer = CountVectorizer()\n",
        "Xtr = vectorizer.fit_transform(X_train); Xte = vectorizer.transform(X_test)\n",
        "mnb = MultinomialNB().fit(Xtr, y_train)\n",
        "pred = mnb.predict(Xte)\n",
        "print('Accuracy:', accuracy_score(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7678d180",
      "metadata": {
        "id": "7678d180"
      },
      "source": [
        "### 27. SVM with different C values – visualize boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b3ff979",
      "metadata": {
        "id": "6b3ff979"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, random_state=0)\n",
        "for C in [0.1, 1, 10]:\n",
        "    clf = SVC(kernel='rbf', C=C).fit(X, y)\n",
        "    x_min, x_max = X[:, 0].min()-1, X[:, 0].max()+1\n",
        "    y_min, y_max = X[:, 1].min()-1, X[:, 1].max()+1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y)\n",
        "    plt.title(f'C={C}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b6af5e5",
      "metadata": {
        "id": "1b6af5e5"
      },
      "source": [
        "### 28. Bernoulli NB on binary-feature dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b083af9b",
      "metadata": {
        "id": "b083af9b"
      },
      "outputs": [],
      "source": [
        "X = np.random.randint(0,2,(300,10))\n",
        "y = np.random.randint(0,2,300)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "bnb = BernoulliNB().fit(X_train,y_train)\n",
        "print('Accuracy:', bnb.score(X_test,y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4231b1e",
      "metadata": {
        "id": "b4231b1e"
      },
      "source": [
        "### 29. Feature scaling before SVM & compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc8d803",
      "metadata": {
        "id": "efc8d803"
      },
      "outputs": [],
      "source": [
        "X = X_wine; y = y_wine\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf_raw = SVC(kernel='rbf').fit(X_tr, y_tr)\n",
        "raw_acc = clf_raw.score(X_te, y_te)\n",
        "sc = StandardScaler()\n",
        "X_trs = sc.fit_transform(X_tr); X_tes = sc.transform(X_te)\n",
        "clf_scaled = SVC(kernel='rbf').fit(X_trs, y_tr)\n",
        "scaled_acc = clf_scaled.score(X_tes, y_tes)\n",
        "print('Raw:', raw_acc, 'Scaled:', scaled_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7130f445",
      "metadata": {
        "id": "7130f445"
      },
      "source": [
        "### 30. Gaussian NB with/without Laplace smoothing (alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "795ff513",
      "metadata": {
        "id": "795ff513"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)\n",
        "gnb = GaussianNB(var_smoothing=1e-9).fit(X_train, y_train)\n",
        "pred1 = gnb.predict(X_test)\n",
        "gnb2 = GaussianNB(var_smoothing=1e-6).fit(X_train, y_train)\n",
        "pred2 = gnb2.predict(X_test)\n",
        "print('Default acc:', accuracy_score(y_test, pred1), 'Higher smoothing acc:', accuracy_score(y_test, pred2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6758ccd0",
      "metadata": {
        "id": "6758ccd0"
      },
      "source": [
        "### 31. GridSearchCV for SVM (C, gamma, kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f218af77",
      "metadata": {
        "id": "f218af77"
      },
      "outputs": [],
      "source": [
        "params = {'C':[0.1,1,10],'gamma':['scale',0.01,0.1],'kernel':['rbf','linear']}\n",
        "grid = GridSearchCV(SVC(), params, cv=3).fit(X_wine, y_wine)\n",
        "print('Best:', grid.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ddeeeda",
      "metadata": {
        "id": "8ddeeeda"
      },
      "source": [
        "### 32. Class weighting on imbalanced data (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b1676a",
      "metadata": {
        "id": "73b1676a"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9,0.1], flip_y=0.01, random_state=42)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "base = SVC().fit(X_tr,y_tr)\n",
        "weighted = SVC(class_weight='balanced').fit(X_tr,y_tr)\n",
        "print('Base:', base.score(X_te,y_te), 'Weighted:', weighted.score(X_te,y_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a733726f",
      "metadata": {
        "id": "a733726f"
      },
      "source": [
        "### 33. Simple Naive Bayes spam detector (toy email data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e54db45",
      "metadata": {
        "id": "3e54db45"
      },
      "outputs": [],
      "source": [
        "corpus = ['win cash now','limited time offer','meeting agenda attached','project update','cheap meds available','earn money fast']\n",
        "y = [1,1,0,0,1,1]  # 1=spam, 0=ham\n",
        "X = CountVectorizer().fit_transform(corpus)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.33,random_state=42)\n",
        "clf = MultinomialNB().fit(X_tr,y_tr)\n",
        "print('Accuracy:', clf.score(X_te,y_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f7df000",
      "metadata": {
        "id": "9f7df000"
      },
      "source": [
        "### 34. SVM vs Naive Bayes on same dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "822f6586",
      "metadata": {
        "id": "822f6586"
      },
      "outputs": [],
      "source": [
        "X, y = X_wine, y_wine\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "svm = SVC().fit(X_tr,y_tr)\n",
        "nb = GaussianNB().fit(X_tr,y_tr)\n",
        "print('SVM:', svm.score(X_te,y_te), 'GNB:', nb.score(X_te,y_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb1f383",
      "metadata": {
        "id": "0eb1f383"
      },
      "source": [
        "### 35. Feature selection before Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95cac95",
      "metadata": {
        "id": "d95cac95"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "X, y = X_wine, y_wine\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "selector = SelectKBest(f_classif, k=8).fit(X_tr,y_tr)\n",
        "X_tr_s = selector.transform(X_tr); X_te_s = selector.transform(X_te)\n",
        "nb1 = GaussianNB().fit(X_tr,y_tr)\n",
        "nb2 = GaussianNB().fit(X_tr_s,y_tr)\n",
        "print('No FS:', nb1.score(X_te,y_te), 'With FS:', nb2.score(X_te_s,y_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d30f922",
      "metadata": {
        "id": "5d30f922"
      },
      "source": [
        "### 36. SVM OvR vs OvO on Wine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8475dff9",
      "metadata": {
        "id": "8475dff9"
      },
      "outputs": [],
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "X, y = X_wine, y_wine\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "ovr = OneVsRestClassifier(SVC()).fit(X_tr,y_tr)\n",
        "ovo = OneVsOneClassifier(SVC()).fit(X_tr,y_tr)\n",
        "print('OvR:', ovr.score(X_te,y_te), 'OvO:', ovo.score(X_te,y_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af63c97d",
      "metadata": {
        "id": "af63c97d"
      },
      "source": [
        "### 37. SVM with Linear, Poly, RBF on Breast Cancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9dcf807",
      "metadata": {
        "id": "b9dcf807"
      },
      "outputs": [],
      "source": [
        "X, y = X_bc, y_bc\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "for k in ['linear','poly','rbf']:\n",
        "    clf = SVC(kernel=k).fit(X_tr,y_tr)\n",
        "    print(k, 'Accuracy:', clf.score(X_te,y_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23cb8cdf",
      "metadata": {
        "id": "23cb8cdf"
      },
      "source": [
        "### 38. SVM with Stratified K-Fold CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51562735",
      "metadata": {
        "id": "51562735"
      },
      "outputs": [],
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(SVC(), X_wine, y_wine, cv=skf)\n",
        "print('Mean accuracy:', scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1b587a",
      "metadata": {
        "id": "ae1b587a"
      },
      "source": [
        "### 39. Naive Bayes with different priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a94daec",
      "metadata": {
        "id": "7a94daec"
      },
      "outputs": [],
      "source": [
        "X, y = X_wine, y_wine\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "priors = [None, [0.7,0.2,0.1]]\n",
        "for p in priors:\n",
        "    g = GaussianNB(priors=p).fit(X_tr,y_tr)\n",
        "    print('Priors',p, 'Acc:', g.score(X_te,y_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6613ef1",
      "metadata": {
        "id": "a6613ef1"
      },
      "source": [
        "### 40. RFE before SVM & compare accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c932214d",
      "metadata": {
        "id": "c932214d"
      },
      "outputs": [],
      "source": [
        "X, y = X_wine, y_wine\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "svc = SVC(kernel='linear')\n",
        "rfe = RFE(estimator=svc, n_features_to_select=8).fit(X_tr,y_tr)\n",
        "X_tr_r, X_te_r = rfe.transform(X_tr), rfe.transform(X_te)\n",
        "svc_base = SVC(kernel='linear').fit(X_tr,y_tr)\n",
        "svc_rfe  = SVC(kernel='linear').fit(X_tr_r,y_tr)\n",
        "print('Base:', svc_base.score(X_te,y_te), 'RFE:', svc_rfe.score(X_te_r,y_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93cf7d16",
      "metadata": {
        "id": "93cf7d16"
      },
      "source": [
        "### 41. Precision, Recall, F1 for SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7401d98",
      "metadata": {
        "id": "a7401d98"
      },
      "outputs": [],
      "source": [
        "X_tr, X_te, y_tr, y_te = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)\n",
        "clf = SVC().fit(X_tr, y_tr)\n",
        "pred = clf.predict(X_te)\n",
        "p, r, f1, _ = precision_recall_fscore_support(y_te, pred, average='binary')\n",
        "print('Precision:', p, 'Recall:', r, 'F1:', f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "858be3a4",
      "metadata": {
        "id": "858be3a4"
      },
      "source": [
        "### 42. Naive Bayes with Log Loss (Cross-Entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8444bf75",
      "metadata": {
        "id": "8444bf75"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)\n",
        "gnb = GaussianNB().fit(X_tr, y_tr)\n",
        "probs = gnb.predict_proba(X_te)\n",
        "print('Log Loss:', log_loss(y_te, probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b92c5a3",
      "metadata": {
        "id": "9b92c5a3"
      },
      "source": [
        "### 43. SVM Confusion Matrix (matplotlib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592c0524",
      "metadata": {
        "id": "592c0524"
      },
      "outputs": [],
      "source": [
        "X_tr, X_te, y_tr, y_te = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)\n",
        "clf = SVC().fit(X_tr, y_tr)\n",
        "pred = clf.predict(X_te)\n",
        "cm = confusion_matrix(y_te, pred)\n",
        "plot_confusion_matrix(cm, classes=['neg','pos'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6849c790",
      "metadata": {
        "id": "6849c790"
      },
      "source": [
        "### 44. SVR with MAE metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5fe4640",
      "metadata": {
        "id": "f5fe4640"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=1000, n_features=6, noise=10, random_state=1)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "reg = SVR().fit(X_tr,y_tr)\n",
        "pred = reg.predict(X_te)\n",
        "print('MAE:', mean_absolute_error(y_te, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d25d181",
      "metadata": {
        "id": "2d25d181"
      },
      "source": [
        "### 45. Naive Bayes ROC-AUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc1f95f",
      "metadata": {
        "id": "4cc1f95f"
      },
      "outputs": [],
      "source": [
        "# Use breast cancer (binary)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)\n",
        "gnb = GaussianNB().fit(X_tr, y_tr)\n",
        "probs = gnb.predict_proba(X_te)[:,1]\n",
        "print('ROC-AUC:', roc_auc_score(y_te, probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f5c1c2",
      "metadata": {
        "id": "f9f5c1c2"
      },
      "source": [
        "### 46. SVM Precision-Recall Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88e06ed0",
      "metadata": {
        "id": "88e06ed0"
      },
      "outputs": [],
      "source": [
        "# Binary conversion: malignant vs benign already binary\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)\n",
        "svm = SVC(probability=True).fit(X_tr, y_tr)\n",
        "probs = svm.predict_proba(X_te)[:,1]\n",
        "prec, rec, _ = precision_recall_curve(y_te, probs)\n",
        "plt.plot(rec, prec)\n",
        "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}